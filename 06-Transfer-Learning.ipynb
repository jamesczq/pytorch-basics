{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7567389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning: take the patterns another model has learned from another problem\n",
    "# and use them for our own problem.\n",
    "\n",
    "# Examples:\n",
    "# Computer vision models leaned from large image dataset such as ImageNet\n",
    "# Large Language Models trained from large amount of text to learn representation of language\n",
    "\n",
    "# Across a wide range of datasets, even if the downstream data of interest appears to only be weakly\n",
    "# related to the data used for pre-training, transfer learning remains the best available option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0c64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Places to find pretrained models:\n",
    "# \n",
    "# PyTorch libraries\n",
    "# e.g., torchvision.models, torchtext.models, torchaudio.models, torchrec.models\n",
    "#\n",
    "# HuggingFace Hub\n",
    "# https://huggingface.co/models\n",
    "# https://huggingface.co/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c96155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Requirement already satisfied: torch in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (2.0.0+cu117)\n",
      "Requirement already satisfied: torchvision in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (0.15.1+cu117)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (2.0.1+cu117)\n",
      "Requirement already satisfied: sympy in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\james\\anaconda3\\envs\\torch\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "torch version: 2.0.0+cu117\n",
      "torchvision version: 0.15.1+cu117\n"
     ]
    }
   ],
   "source": [
    "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b6cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ef290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_05_modular import data_setup, engine, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f08cd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = utils.get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0795ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "676f9991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51bf4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(\"data/\")\n",
    "image_path = data_path/\"pizza_steak_sushi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f053f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = image_path/\"train\"\n",
    "test_dir = image_path/\"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6de2d0",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934cbb6",
   "metadata": {},
   "source": [
    "### 1. Create Transforms for torchvision.models: manual creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de879393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using a pre-trained model, your custom data must be prepared the same way \n",
    "# as the original training data that went into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "966b0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifically, computer vision models in torchvision expect input images normalized in the following way:\n",
    "# * Mini-batches of three-channel RGB images of shape 3*Height*Width, with H, W at least 224.\n",
    "# * Images have to be loaded in to a range of [0, 1], and then normalize using\n",
    "# mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# The above steps in PyTorch code are:\n",
    "# 1. Mini-batches of size [batch_size, 3, height, width], using\n",
    "# torchvision.transforms.Resize() and then torch.utils.data.DataLoader() to create batches\n",
    "# 2. Values between 0 and 1, using\n",
    "# torchvision.transforms.ToTensor()\n",
    "# 3. Normalize using\n",
    "# torchvision.transforms.Normalize(mean=, std=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51c29778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "manual_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab9d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d23798bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x256cea46910>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x256cea461f0>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc51e8",
   "metadata": {},
   "source": [
    "### 2. Create Transforms for torchvision.models: auto creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4b724f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As of torchvision v0.13+, auto transform creation feature has been added.\n",
    "# Say you'd like to use the following model: ModelABC, you can obtain\n",
    "# weights = torchvision.models.ModelABC.DEFAULT \n",
    "# (DEFAULT indicates the best available weights for the chosen model architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3cfbcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet_B0_Weights.IMAGENET1K_V1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a632de5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, access the transforms associated with the above model's weights\n",
    "auto_transforms = weights.transforms()\n",
    "auto_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90916d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same way for train/test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0dc0569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x256cea46040>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x256cea46580>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=auto_transforms,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834504d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
